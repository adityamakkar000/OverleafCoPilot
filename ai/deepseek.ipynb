{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adityamakkar/anaconda3/envs/overleafcopilot/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_name = \"deepseek-ai/deepseek-math-7b-instruct\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "# model.generation_config = GenerationConfig.from_pretrained(model_name)\n",
    "# model.generation_config.pad_token_id = model.generation_config.eos_token_id\n",
    "\n",
    "question = r\"\"\"\\item \\label{affine.subset} Let $V$ be a vector space. A \\emph{subset} $H$ of $V$ is called \\emph{affine} if there exists a subspace $W$ of $V$ and a point $\\vx_0 \\in H$ such that\n",
    "\\begin{equation} \\label{eq:affine}\n",
    "H = \\{ \\vx_0 + \\vw : \\vw \\in W \\}.\n",
    "\\end{equation}\n",
    "We say that $H$ is an affine subset \\emph{modelled} on the subspace $W$.\n",
    "\n",
    "\\textbf{Remark.} Some authors use the term ``affine subspace'' to denote an affine subset and ``linear subspace'' to denote the usual notion of subspace. We use affine subset to avoid confusion.\n",
    "\\begin{enumerate}[{$[$}a{$]$}]\n",
    "\\item Show that the point $\\vx_0$ is \\emph{not} unique. That is, show $H = \\{ \\vx'_0 + \\vw : \\vw \\in W \\}$ for any $\\vx_0' \\in H$.\n",
    "\"\"\"\n",
    "\n",
    "statement = r\"\"\"\n",
    "\n",
    "We need to show that if $H = \\{ x_0 + w: w \\in W\\}$ then $H = \\{ x_0' + w: w \\in W\\}$ for any $x_0' \\in H$. Then we have two cases. $W$ is the $0$ subspace or $W$ is a subspace with more than 1 element.\n",
    "If $W$ is a $0$   \"\"\"\n",
    "instruction = f\"There will be a quesiton and an incomplete proof. Please reason step by step to help complete the latex proof and put your final latex sentence within \\\\boxed{{}}. The question is {question}. The statement is: \\n\"\n",
    "messages = [\n",
    "    {\"role\": \"user\",\n",
    "     \"content\": f\"\"\"{instruction} {statement}\"\"\"\n",
    "    }]\n",
    "\n",
    "# input_tensor = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\")\n",
    "# outputs = model.generate(input_tensor.to(model.device), max_new_tokens=input_tensor.shape[1] + 150)\n",
    "\n",
    "# result = tokenizer.decode(outputs[0][input_tensor.shape[1]:], skip_special_tokens=True)\n",
    "# print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 0 examples [00:00, ? examples/s]/Users/adityamakkar/anaconda3/envs/overleafcopilot/lib/python3.11/site-packages/datasets/download/streaming_download_manager.py:784: FutureWarning: The 'verbose' keyword in pd.read_csv is deprecated and will be removed in a future version.\n",
      "  return pd.read_csv(xopen(filepath_or_buffer, \"rb\", download_config=download_config), **kwargs)\n",
      "Generating train split: 126195 examples [00:00, 236088.57 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import List, Dict\n",
    "import datasets as d\n",
    "import csv\n",
    "from omegaconf import MISSING\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DatasetConfig:\n",
    "    instruction: str = MISSING\n",
    "    inputPath: str = MISSING\n",
    "    outputPath: str = MISSING\n",
    "    cutoff: int = 7\n",
    "\n",
    "\n",
    "class DatasetProcessor:\n",
    "    def __init__(self, config: DatasetConfig):\n",
    "        self.config = config\n",
    "\n",
    "    def read_data(self) -> List[str]:\n",
    "        with open(self.config.inputPath, \"r\") as file:\n",
    "            data = file.read()\n",
    "        return self._preprocess_data(data)\n",
    "\n",
    "    def _preprocess_data(self, data: str) -> List[str]:\n",
    "        sentences = data.split(\".\")\n",
    "        return [\n",
    "            x.strip(\"\\n\\\\n\").replace(\"\\n\", \" \").replace(\"  \", \" \") for x in sentences\n",
    "        ]\n",
    "\n",
    "    def create_dataset(self, data: List[str]) -> List[Dict]:\n",
    "        dataset = []\n",
    "        length = len(data)\n",
    "        for i in range(length):\n",
    "            sentence = data[i]\n",
    "            if len(sentence) - 1 < self.config.cutoff:\n",
    "                continue\n",
    "\n",
    "            indexes = [x for x in range(self.config.cutoff, len(sentence) - 1)]\n",
    "            input = [sentence[i:_] for _ in indexes]\n",
    "            output = [sentence[_:] for _ in indexes]\n",
    "            for a, b in zip(input, output):\n",
    "                dataset.append(\n",
    "                    {\n",
    "                        \"instruction\": f\"{self.config.instruction}\",\n",
    "                        \"input\": r\"{}\".format(a),\n",
    "                        \"output\": r\"{}\".format(b),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        return dataset\n",
    "\n",
    "    def save_dataset(self, dataset: List[Dict]) -> None:\n",
    "        with open(\n",
    "            self.config.outputPath, mode=\"w\", newline=\"\", encoding=\"utf-8\"\n",
    "        ) as file:\n",
    "            writer = csv.DictWriter(file, fieldnames=[\"instruction\", \"input\", \"output\"])\n",
    "            writer.writeheader()\n",
    "            writer.writerows(dataset)\n",
    "\n",
    "\n",
    "def process_dataset(config: DatasetConfig) -> None:\n",
    "    processor = DatasetProcessor(config)\n",
    "    data = processor.read_data()\n",
    "    dataset = processor.create_dataset(data)\n",
    "    processor.save_dataset(dataset)\n",
    "\n",
    "\n",
    "def get_dataset(config: DatasetConfig):\n",
    "    process_dataset(config)\n",
    "    ds = d.load_dataset(config.outputPath[-3:], data_files=config.outputPath)\n",
    "    return ds\n",
    "\n",
    "\n",
    "dataCFG = {\n",
    "    \"instruction\": instruction,\n",
    "    \"inputPath\": \"./dataset/latex.txt\",\n",
    "    \"outputPath\": \"./dataset/latex.csv\",\n",
    "    \"cutoff\": 7,\n",
    "}\n",
    "\n",
    "cfg = DatasetConfig(\n",
    "    instruction=dataCFG[\"instruction\"],\n",
    "    inputPath=dataCFG[\"inputPath\"],\n",
    "    outputPath=dataCFG[\"outputPath\"],\n",
    ")\n",
    "\n",
    "d = get_dataset(cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import LoraConfig, PeftModel, get_peft_model\n",
    "from trl import SFTTrainer\n",
    "import hydra\n",
    "from hydra.core.config_store import ConfigStore\n",
    "from omegaconf import OmegaConf, MISSING\n",
    "from ds import get_dataset, DatasetConfig\n",
    "from dataclasses import dataclass\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "@dataclass\n",
    "class ftConfig:\n",
    "    model_id: str = MISSING\n",
    "    precision: str = \"bfloat16\"\n",
    "    seed: int = MISSING\n",
    "    test_size: float = MISSING\n",
    "    modules_limit: int = MISSING\n",
    "    r: int = MISSING\n",
    "    lora_alpha: int = MISSING\n",
    "    dataCFG: DatasetConfig = MISSING\n",
    "    prefix_txt: str = MISSING\n",
    "    per_device_train_batch_size: int = MISSING\n",
    "    gradient_accumulation_steps: int = MISSING\n",
    "    optim: str = MISSING\n",
    "    warmup_steps: float = MISSING\n",
    "    max_steps: int = MISSING\n",
    "    learning_rate: float = MISSING\n",
    "    logging_steps: int = MISSING\n",
    "    output: str = MISSING\n",
    "    name: str = MISSING\n",
    "    overwrite: bool = MISSING\n",
    "\n",
    "class ModelTrainer:\n",
    "    def __init__(self, cfg: ftConfig):\n",
    "        self.cfg = cfg\n",
    "        self.device = self._get_device()\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "\n",
    "    def _get_device(self):\n",
    "        if torch.backends.mps.is_available():\n",
    "            return \"mps\"\n",
    "        return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def setup_output_directory(self):\n",
    "        if os.path.exists(f\"{self.cfg.output}/{self.cfg.name}/config.yaml\"):\n",
    "            if self.cfg.overwrite:\n",
    "                shutil.rmtree(f\"{self.cfg.output}/{self.cfg.name}/\")\n",
    "            else:\n",
    "                raise FileExistsError(\n",
    "                    \"Output directory exists. Set overwrite to true to overwrite.\"\n",
    "                )\n",
    "\n",
    "        os.makedirs(f\"./{self.cfg.output}/{self.cfg.name}/\", exist_ok=True)\n",
    "        with open(f\"./{self.cfg.output}/{self.cfg.name}/config.yaml\", \"w\") as file:\n",
    "            file.write(str(self.cfg))\n",
    "\n",
    "    def prepare_dataset(self):\n",
    "        def generate_prompt(data_point, tokenizer):\n",
    "            message =  [{\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"\"\"{data_point[\"instruction\"]} {data_point[\"input\"]}\"\"\"\n",
    "            },{\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": data_point[\"output\"]\n",
    "            }\n",
    "                        ]\n",
    "\n",
    "            prompt = tokenizer.apply_chat_template(message, tokenize=False)\n",
    "            tokenized_prompt = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "            text = {\n",
    "                'prompt': prompt,\n",
    "                **tokenized_prompt\n",
    "            }\n",
    "            return text\n",
    "\n",
    "        ds = get_dataset(self.cfg.dataCFG)[\"train\"]\n",
    "        ds = ds.map(lambda samples: generate_prompt(samples, self.tokenizer), batched=False)\n",
    "        ds = ds.shuffle(seed=self.cfg.seed)\n",
    "        ds = ds.train_test_split(test_size=self.cfg.test_size)\n",
    "\n",
    "        return ds[\"train\"], ds[\"test\"]\n",
    "\n",
    "    def setup_model(self):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            self.cfg.model_id, add_eos_token=True, padding_side=\"left\"\n",
    "        )\n",
    "\n",
    "        if self.cfg.precision == \"bfloat16\":\n",
    "            dp = torch.bfloat16\n",
    "        elif self.cfg.precision == \"float32\":\n",
    "            dp = torch.float32\n",
    "        elif self.cfg.precision == \"float16\":\n",
    "            dp = torch.float16\n",
    "        else:\n",
    "            raise ValueError(\"Invalid precision value.\")\n",
    "\n",
    "\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.cfg.model_id,torch_dtype=dp , device_map=self.device\n",
    "        )\n",
    "        print(f\"Model {self.cfg.model_id} loaded successfully on {self.device} @ {dp} precision.\")\n",
    "\n",
    "\n",
    "        modules = self._find_all_linear_names()\n",
    "        target_modules = (\n",
    "            modules\n",
    "            if len(modules) < self.cfg.modules_limit\n",
    "            else modules[: self.cfg.modules_limit]\n",
    "        )\n",
    "\n",
    "        lora_config = LoraConfig(\n",
    "            r=self.cfg.r,\n",
    "            lora_alpha=self.cfg.lora_alpha,\n",
    "            target_modules=target_modules,\n",
    "            lora_dropout=0.05,\n",
    "            bias=\"none\",\n",
    "            task_type=\"CAUSAL_LM\",\n",
    "        )\n",
    "\n",
    "        self.model = get_peft_model(self.model, lora_config)\n",
    "        return lora_config\n",
    "\n",
    "    def _find_all_linear_names(self):\n",
    "        lora_module_names = set()\n",
    "        for name, module in self.model.named_modules():\n",
    "            if isinstance(module, torch.nn.Linear):\n",
    "                names = name.split(\".\")\n",
    "                lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "        return list(lora_module_names)\n",
    "\n",
    "    def train(self):\n",
    "        self.setup_output_directory()\n",
    "        lora_config = self.setup_model()\n",
    "        train_data, test_data = self.prepare_dataset()\n",
    "\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        self.tokenizer.padding_side = \"right\"\n",
    "\n",
    "        trainer = SFTTrainer(\n",
    "            model=self.model,\n",
    "            train_dataset=train_data,\n",
    "            eval_dataset=test_data,\n",
    "            dataset_text_field=\"prompt\",\n",
    "            peft_config=lora_config,\n",
    "            max_seq_length=250,\n",
    "            args=transformers.TrainingArguments(\n",
    "                per_device_train_batch_size=self.cfg.per_device_train_batch_size,\n",
    "                gradient_accumulation_steps=self.cfg.gradient_accumulation_steps,\n",
    "                warmup_steps=self.cfg.warmup_steps,\n",
    "                max_steps=self.cfg.max_steps,\n",
    "                learning_rate=self.cfg.learning_rate,\n",
    "                logging_steps=self.cfg.logging_steps,\n",
    "                output_dir=f\"{self.cfg.output}/{self.cfg.name}/checkpoints\",\n",
    "                optim=self.cfg.optim,\n",
    "                save_strategy=\"epoch\",\n",
    "            ),\n",
    "            data_collator=transformers.DataCollatorForLanguageModeling(\n",
    "                self.tokenizer, mlm=False\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        trainer.train()\n",
    "        self.save_models(trainer)\n",
    "\n",
    "    def save_models(self, trainer):\n",
    "        new_model_path = f\"{self.cfg.output}/{self.cfg.name}/finetuned_models/\"\n",
    "        trainer.model.save_pretrained(new_model_path)\n",
    "\n",
    "        merged_model = PeftModel.from_pretrained(self.model, new_model_path)\n",
    "        merged_model = merged_model.merge_and_unload()\n",
    "\n",
    "        merged_path = f\"{self.cfg.output}/{self.cfg.name}/merged_models/\"\n",
    "        merged_model.save_pretrained(merged_path)\n",
    "        self.tokenizer.save_pretrained(merged_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "deepseek_cfg = {\n",
    "    \"model_id\": \"google/gemma-2b-it\",\n",
    "    \"seed\": 42,\n",
    "    \"test_size\": 0.1,\n",
    "    \"modules_limit\": 10,\n",
    "    \"r\": 1,\n",
    "    \"lora_alpha\": 1,\n",
    "    \"dataCFG\": cfg,\n",
    "    \"per_device_train_batch_size\": 1,\n",
    "    \"gradient_accumulation_steps\": 1,\n",
    "    \"optim\": \"adamw_torch\",\n",
    "    \"warmup_steps\": 0.1,\n",
    "    \"max_steps\": 100,\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"logging_steps\": 10,\n",
    "    \"output\": \"./output\",\n",
    "    \"name\": \"gemma-2b\",\n",
    "    \"overwrite\": True,\n",
    "}\n",
    "deepseek_cfg = ftConfig(\n",
    "  **deepseek_cfg\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adityamakkar/anaconda3/envs/overleafcopilot/lib/python3.11/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model google/gemma-2b-it loaded successfully on mps @ torch.bfloat16 precision.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 0 examples [00:00, ? examples/s]/Users/adityamakkar/anaconda3/envs/overleafcopilot/lib/python3.11/site-packages/datasets/download/streaming_download_manager.py:784: FutureWarning: The 'verbose' keyword in pd.read_csv is deprecated and will be removed in a future version.\n",
      "  return pd.read_csv(xopen(filepath_or_buffer, \"rb\", download_config=download_config), **kwargs)\n",
      "Generating train split: 126195 examples [00:00, 190335.09 examples/s]\n",
      "Map: 100%|██████████| 126195/126195 [00:34<00:00, 3685.34 examples/s]\n",
      "/Users/adityamakkar/anaconda3/envs/overleafcopilot/lib/python3.11/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 113575/113575 [00:07<00:00, 14575.30 examples/s]\n",
      "Map: 100%|██████████| 12620/12620 [00:00<00:00, 14154.54 examples/s]\n",
      "/Users/adityamakkar/anaconda3/envs/overleafcopilot/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:290: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n",
      " 10%|█         | 10/100 [00:09<01:09,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.475, 'grad_norm': 2.0625, 'learning_rate': 9.009009009009008e-05, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 20/100 [00:17<01:00,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.8328, 'grad_norm': 3.375, 'learning_rate': 8.008008008008008e-05, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 30/100 [00:24<00:52,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0992, 'grad_norm': 4.34375, 'learning_rate': 7.007007007007007e-05, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 40/100 [00:32<00:45,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5539, 'grad_norm': 5.0625, 'learning_rate': 6.0060060060060066e-05, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 50/100 [00:39<00:37,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1508, 'grad_norm': 1.6328125, 'learning_rate': 5.0050050050050046e-05, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 60/100 [00:47<00:30,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8266, 'grad_norm': 1.625, 'learning_rate': 4.004004004004004e-05, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 70/100 [00:54<00:22,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5766, 'grad_norm': 1.09375, 'learning_rate': 3.0030030030030033e-05, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 80/100 [01:02<00:15,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.423, 'grad_norm': 0.921875, 'learning_rate': 2.002002002002002e-05, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 90/100 [01:10<00:07,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3391, 'grad_norm': 1.484375, 'learning_rate': 1.001001001001001e-05, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:17<00:00,  1.32it/s]/Users/adityamakkar/anaconda3/envs/overleafcopilot/lib/python3.11/site-packages/peft/utils/save_and_load.py:134: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3146, 'grad_norm': 1.6015625, 'learning_rate': 0.0, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:18<00:00,  1.27it/s]\n",
      "/Users/adityamakkar/anaconda3/envs/overleafcopilot/lib/python3.11/site-packages/peft/utils/save_and_load.py:134: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 78.5995, 'train_samples_per_second': 1.272, 'train_steps_per_second': 1.272, 'train_loss': 1.35916015625, 'epoch': 0.0}\n"
     ]
    }
   ],
   "source": [
    "trainer = ModelTrainer(deepseek_cfg)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adityamakkar/anaconda3/envs/overleafcopilot/lib/python3.11/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.86s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "@dataclass\n",
    "class InferenceConfig:\n",
    "    path: str = MISSING\n",
    "\n",
    "class ModelManager:\n",
    "    def __init__(self, cfg: ftConfig):\n",
    "        self.device = self._get_device()\n",
    "        self.model, self.tokenizer = self.load_model(cfg)\n",
    "        self.device = self._get_device()\n",
    "        self.instruction = cfg.dataCFG.instruction\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_device():\n",
    "        if torch.cuda.is_available():\n",
    "            return \"cuda\"\n",
    "        if torch.backends.mps.is_available():\n",
    "            return \"mps\"\n",
    "        return \"cpu\"\n",
    "\n",
    "    def load_model(self, cfg: InferenceConfig):\n",
    "        model = AutoModelForCausalLM.from_pretrained(cfg.model_id,\n",
    "                                                     torch_dtype=torch.bfloat16,\n",
    "                                                     device_map=self.device)\n",
    "        merged_model = PeftModel.from_pretrained(\n",
    "            model, f\"{cfg.output}/{cfg.name}/finetuned_models/\"\n",
    "        )\n",
    "        merged_model = merged_model.merge_and_unload()\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            cfg.model_id, add_eos_token=True, padding_side=\"left\"\n",
    "        )\n",
    "        return merged_model, tokenizer\n",
    "\n",
    "    def generate_prompt(self, input: str) -> str:\n",
    "        message = [{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"{self.instruction} {input}\"\n",
    "        }]\n",
    "        print(message)\n",
    "\n",
    "        return self.tokenizer.apply_chat_template(message, add_generation_prompt=True, return_tensors=\"pt\")\n",
    "\n",
    "    def __call__(self, input: str) -> str:\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        self.tokenizer.padding_side = \"left\"\n",
    "\n",
    "        input_tensor = self.generate_prompt(input)\n",
    "        print(input_tensor)\n",
    "        outputs = self.model.generate(\n",
    "            input_tensor.to(self.device),\n",
    "            max_new_tokens=100,\n",
    "            do_sample=True,\n",
    "            pad_token_id=self.tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "        text = self.tokenizer.decode(outputs[0][input_tensor.shape[1]:], skip_special_tokens=True)\n",
    "        return text\n",
    "\n",
    "def setup_config_store():\n",
    "    cs = ConfigStore.instance()\n",
    "    cs.store(name=\"inference_config\", node=InferenceConfig)\n",
    "\n",
    "\n",
    "model_manager = ModelManager(deepseek_cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'user', 'content': \"There will be a quesiton and an incomplete proof. Please reason step by step to help complete the latex proof and put your final latex sentence within \\\\boxed{}. The question is \\\\item \\\\label{affine.subset} Let $V$ be a vector space. A \\\\emph{subset} $H$ of $V$ is called \\\\emph{affine} if there exists a subspace $W$ of $V$ and a point $\\\\vx_0 \\\\in H$ such that\\n\\\\begin{equation} \\\\label{eq:affine}\\nH = \\\\{ \\\\vx_0 + \\\\vw : \\\\vw \\\\in W \\\\}.\\n\\\\end{equation}\\nWe say that $H$ is an affine subset \\\\emph{modelled} on the subspace $W$.\\n\\n\\\\textbf{Remark.} Some authors use the term ``affine subspace'' to denote an affine subset and ``linear subspace'' to denote the usual notion of subspace. We use affine subset to avoid confusion.\\n\\\\begin{enumerate}[{$[$}a{$]$}]\\n\\\\item Show that the point $\\\\vx_0$ is \\\\emph{not} unique. That is, show $H = \\\\{ \\\\vx'_0 + \\\\vw : \\\\vw \\\\in W \\\\}$ for any $\\\\vx_0' \\\\in H$.\\n. The statement is: \\n Pick any point $s$ in $H$ since it is not empty. Then we have to show that the set $S = \\\\{h - s: h \\\\in H \\\\}$ is a linear subspace. Then $0 \\\\in S$ since when $h = s $. For scalar multiplication we have to show that if $h - s \\\\in S$ then $c(h-s) \\\\in S$ where $c \\\\in F$. From the CL property we have $(1-c)s + ch \\\\in F$. Thus we have $b := s + c(h-s) \\\\in H$. Thus we have $b - s \\\\in S$ by definition of $S$ and thus $c(h-s) \\\\in S$. To show scalar addition if we have $u = h - s, v = h' - s \\\\in S$ then $u + v \\\\in S$. Adding both we have $u + v = h + h' - 2s$. Thus to show that this is in $S$ we need to show that $h + h' - s \\\\in H$. From CL and the fact that $char F \\\\neq 2$ then that means $(1-2)s + 2h = 2h -s\\\\in H$ and $(1-2)s + 2h' = 2h' - s\\\\in H$. Thus since $2 \\\\in F \\\\ \"}]\n",
      "tensor([[     2,    106,   1645,    108,   3493,    877,    614,    476,   1856,\n",
      "          35184,    578,    671,  38828,   9102, 235265,   5651,   3519,   4065,\n",
      "            731,   4065,    577,   1707,   3407,    573,  57904,   9102,    578,\n",
      "           2507,    861,   2048,  57904,  13060,   2819,    730, 111983, 235282,\n",
      "           8331,    714,   2872,    603,    730,    884,    730,   2097, 235282,\n",
      "         178250, 235265,  35629, 235270,   4371,    697, 235330, 235323,    614,\n",
      "            476,   4301,   3641, 235265,    586,    730,  33534, 235282,  35629,\n",
      "         235270,    697, 235314, 235323,    576,    697, 235330, 235323,    603,\n",
      "           3151,    730,  33534, 235282, 178250, 235270,   1013,   1104,  10489,\n",
      "            476, 110621,    697, 235325, 235323,    576,    697, 235330, 235323,\n",
      "            578,    476,   2377,   1467,  48863, 235298, 235276,    730,    473,\n",
      "            640, 235323,   1582,    674,    108, 235286,    902, 235282,   5433,\n",
      "         235270,    730,   2097, 235282,   5795, 235292, 178250, 235270,    108,\n",
      "         235314,    589,  44478,    730,  48863, 235298, 235276,    963,    730,\n",
      "          32076,    865,    730,  32076,    730,    473,    647,    730,   8331,\n",
      "            108, 235286,    615, 235282,   5433, 235270,    108,   1734,   1931,\n",
      "            674,    697, 235314, 235323,    603,    671, 107078,  38397,    730,\n",
      "          33534, 235282,   2516,   1060, 235270,    611,    573, 110621,    697,\n",
      "         235325,   3306,    109, 235286,  36218, 235282,  40564,  10403,   4213,\n",
      "          11646,   1281,    573,   5168,  30554, 178250, 110621,   3404,    577,\n",
      "          36131,    671, 107078,  38397,    578,  30554,  15842, 110621,   3404,\n",
      "            577,  36131,    573,  11653,  25777,    576, 110621, 235265,   1448,\n",
      "           1281, 107078,  38397,    577,   6858,  23186, 235265,    108, 235286,\n",
      "            902, 235282,   1473,  13434,  12563,  11894, 235270, 235250,  12563,\n",
      "          16293,  12765,    108, 235286,    884,   7683,    674,    573,   2377,\n",
      "           1467,  48863, 235298, 235276, 235323,    603,    730,  33534, 235282,\n",
      "           1665, 235270,   6239, 235265,   2995,    603, 235269,   1500,    697,\n",
      "         235314,    589,  44478,    730,  48863,  45139, 235276,    963,    730,\n",
      "          32076,    865,    730,  32076,    730,    473,    647,    730,   1208,\n",
      "            604,   1089,   1467,  48863, 235298, 235276, 235303,    730,    473,\n",
      "            640,   3306,    108, 235265,    714,   6218,    603, 235292, 235248,\n",
      "            108,  17350,   1089,   2377,    697, 235256, 235323,    575,    697,\n",
      "         235314, 235323,   2754,    665,    603,    780,   8144, 235265,   5040,\n",
      "            783,    791,    577,   1500,    674,    573,   1142,    697, 235277,\n",
      "            589,  44478, 235259,    728,    485, 235292,    531,    730,    473,\n",
      "            640,    730,   1208,    603,    476,  10983, 110621, 235265,   5040,\n",
      "            697, 235276,    730,    473,    570, 235323,   2754,   1185,    697,\n",
      "         235259,    589,    485,  16484,   1699,  33591,  46824,    783,    791,\n",
      "            577,   1500,    674,   1013,    697, 235259,    728,    485,    730,\n",
      "            473,    570, 235323,   1492,    697, 235260, 235278, 235259, 235290,\n",
      "         235256, 235275,    730,    473,    570, 235323,   1570,    697, 235260,\n",
      "            730,    473,    633,   3306,   4845,    573,   7425,   4227,    783,\n",
      "            791,   6054, 235274, 235290, 235260, 235275, 235256,    963,    788,\n",
      "            730,    473,    633,   3306,   8707,    783,    791,    697, 235268,\n",
      "           3527,    485,    963,    498, 235278, 235259, 235290, 235256, 235275,\n",
      "            730,    473,    640,   3306,   8707,    783,    791,    697, 235268,\n",
      "            728,    485,    730,    473,    570, 235323,    731,  10165,    576,\n",
      "            697, 235277, 235323,    578,   6319,    697, 235260, 235278, 235259,\n",
      "         235290, 235256, 235275,    730,    473,    570,   3306,   1887,   1500,\n",
      "          33591,   5081,   1013,    783,    791,    697, 235261,    589,    531,\n",
      "            728,    485, 235269,    593,    589,    531, 235303,    728,    485,\n",
      "            730,    473,    570, 235323,   1492,    697, 235261,    963,    593,\n",
      "            730,    473,    570,   3306,  47228,   2145,    783,    791,    697,\n",
      "         235261,    963,    593,    589,    531,    963,    531, 235303,    728,\n",
      "         235248, 235284, 235256,   3306,   8707,    577,   1500,    674,    736,\n",
      "            603,    575,    697, 235277, 235323,    783,   1476,    577,   1500,\n",
      "            674,    697, 235259,    963,    531, 235303,    728,    485,    730,\n",
      "            473,    640,   3306,   4845,   7425,    578,    573,   2252,    674,\n",
      "            697,   2288,    633,    730,  27953, 235248, 235284, 235323,   1492,\n",
      "            674,   3454,   6054, 235274, 235290, 235284, 235275, 235256,    963,\n",
      "         235248, 235284, 235259,    589, 235248, 235284, 235259,    728, 235256,\n",
      "         235286,    473,    640, 235323,    578,   6054, 235274, 235290, 235284,\n",
      "         235275, 235256,    963, 235248, 235284, 235259, 235303,    589, 235248,\n",
      "         235284, 235259, 235303,    728,    485, 235286,    473,    640,   3306,\n",
      "           8707,   2754,    697, 235284,    730,    473,    633,    730,    107,\n",
      "            108,    106,   2516,    108]])\n",
      "We have to show that the point $\\vx_0$ is \\emph{not} unique. That is, show $H = \\{ \\vx'_0 + \\vw : \\vw \\in W \\}$ for any $\\vx_0' \\in H$.\n",
      "\n",
      "Pick any point $s$ in $H$ since it is not empty. Then we have to show that the set $S = \\{h - s: h \\in H \\}$ is a linear subspace. Then $\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prompt_input = r\"\"\"Pick any point $s$ in $H$ since it is not empty. Then we have to show that the set $S = \\{h - s: h \\in H \\}$ is a linear subspace. Then $0 \\in S$ since when $h = s $. For scalar multiplication we have to show that if $h - s \\in S$ then $c(h-s) \\in S$ where $c \\in F$. From the CL property we have $(1-c)s + ch \\in F$. Thus we have $b := s + c(h-s) \\in H$. Thus we have $b - s \\in S$ by definition of $S$ and thus $c(h-s) \\in S$. To show scalar addition if we have $u = h - s, v = h' - s \\in S$ then $u + v \\in S$. Adding both we have $u + v = h + h' - 2s$. Thus to show that this is in $S$ we need to show that $h + h' - s \\in H$. From CL and the fact that $char F \\neq 2$ then that means $(1-2)s + 2h = 2h -s\\in H$ and $(1-2)s + 2h' = 2h' - s\\in H$. Thus since $2 \\in F \\ \"\"\"\n",
    "print(model_manager(prompt_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'user', 'content': \"There will be a quesiton and an incomplete proof. Please reason step by step to help complete the latex proof and put your final latex sentence within \\\\boxed{}. The question is \\\\item \\\\label{affine.subset} Let $V$ be a vector space. A \\\\emph{subset} $H$ of $V$ is called \\\\emph{affine} if there exists a subspace $W$ of $V$ and a point $\\\\vx_0 \\\\in H$ such that\\n\\\\begin{equation} \\\\label{eq:affine}\\nH = \\\\{ \\\\vx_0 + \\\\vw : \\\\vw \\\\in W \\\\}.\\n\\\\end{equation}\\nWe say that $H$ is an affine subset \\\\emph{modelled} on the subspace $W$.\\n\\n\\\\textbf{Remark.} Some authors use the term ``affine subspace'' to denote an affine subset and ``linear subspace'' to denote the usual notion of subspace. We use affine subset to avoid confusion.\\n\\\\begin{enumerate}[{$[$}a{$]$}]\\n\\\\item Show that the point $\\\\vx_0$ is \\\\emph{not} unique. That is, show $H = \\\\{ \\\\vx'_0 + \\\\vw : \\\\vw \\\\in W \\\\}$ for any $\\\\vx_0' \\\\in H$.\\n. The statement is: \\n Pick any point $s$ in $H$ since it is not empty. Then we have to show that the set $S = \\\\{h - s: h \\\\in H \\\\}$ is a linear subspace. Then $0 \\\\in S$ since when $h = s $. For scalar multiplication we have to show that if $h - s \\\\in S$ then $c(h-s) \\\\in S$ where $c \\\\in F$. From the CL property we have $(1-c)s + ch \\\\in F$. Thus we have $b := s + c(h-s) \\\\in H$. Thus we have $b - s \\\\in S$ by definition of $S$ and thus $c(h-s) \\\\in S$. To show scalar addition if we have $u = h - s, v = h' - s \\\\in S$ then $u + v \\\\in S$. Adding both we have $u + v = h + h' - 2s$. Thus to show that this is in $S$ we need to show that $h + h' - s \\\\in H$. From CL and the fact that $char F \\\\neq 2$ then that means $(1-2)s + 2h = 2h -s\\\\in H$ and $(1-2)s + 2h' = 2h' - s\\\\in H$. Thus since $2 \\\\in F \\\\ \"}]\n",
      "tensor([[100000,   5726,     25,   2071,    543,    330,    245,    445,    257,\n",
      "          35236,    285,    274,  29444,   5637,     13,   6456,   2806,   3458,\n",
      "            457,   3458,    276,   1345,   3938,    254,  61310,   5637,    285,\n",
      "           1957,    520,   2328,  61310,   4976,   2383,    357,  63962,     90,\n",
      "           1424,    429,   2512,    317,    357,   2013,    357,   1208,     90,\n",
      "          86912,     13,   6024,     92,   3960,    363,     53,      3,    330,\n",
      "            245,   6133,   2516,     13,    338,    357,   4533,     90,   6024,\n",
      "             92,    363,     39,      3,    280,    363,     53,      3,    317,\n",
      "           2424,    357,   4533,     90,  86912,     92,    565,    745,   6057,\n",
      "            245,  28918,    363,     54,      3,    280,    363,     53,      3,\n",
      "            285,    245,   1420,    628,  51540,     62,     15,    357,    246,\n",
      "            415,      3,   1108,    344,    185,     59,    951,     90,   1441,\n",
      "             92,    357,   1208,     90,   1187,     25,  86912,     92,    185,\n",
      "             39,    403,   8138,    357,  51540,     62,     15,    919,    357,\n",
      "          57236,   1193,    357,  57236,    357,    246,    423,  89483,    185,\n",
      "             59,    409,     90,   1441,     92,    185,   1380,   1481,    344,\n",
      "            363,     39,      3,    317,    274,  33479,  12030,    357,   4533,\n",
      "             90,   1794,   6677,     92,    331,    254,  28918,    363,     54,\n",
      "           1332,    185,    185,     59,   4051,     90,  46882,   4992,   4754,\n",
      "          10598,    938,    254,   1639,   6078,  86912,  28918,   4255,    276,\n",
      "           9211,    274,  33479,  12030,    285,   6078,   9263,  28918,   4255,\n",
      "            276,   9211,    254,   7618,  15148,    280,  28918,     13,   1003,\n",
      "            938,  33479,  12030,    276,   4945,  16305,     13,    185,     59,\n",
      "            951,     90,   9751,   3669,  11862,     58,   9938,     64,  11862,\n",
      "           8826,   7175,    185,     59,   2013,  11091,    344,    254,   1420,\n",
      "            628,  51540,     62,     15,      3,    317,    357,   4533,     90,\n",
      "           1265,     92,   4730,     13,   2608,    317,     11,   1296,    363,\n",
      "             39,    403,   8138,    357,  51540,  13223,     15,    919,    357,\n",
      "          57236,   1193,    357,  57236,    357,    246,    423,  30446,    327,\n",
      "            688,    628,  51540,     62,     15,      6,    357,    246,    415,\n",
      "           1332,    185,     13,    429,   6161,    317,     25,    207,    185,\n",
      "          22982,    688,   1420,    363,     82,      3,    279,    363,     39,\n",
      "              3,   1962,    359,    317,    441,   7137,     13,   2928,    395,\n",
      "            463,    276,   1296,    344,    254,    845,    363,     50,    403,\n",
      "           8138,     71,    570,    252,     25,    286,    357,    246,    415,\n",
      "          30446,    317,    245,   6312,  28918,     13,   2928,    363,     15,\n",
      "            357,    246,    324,      3,   1962,    754,    363,     71,    403,\n",
      "            252,  10330,   1494,  14115,  30313,    395,    463,    276,   1296,\n",
      "            344,    565,    363,     71,    570,    252,    357,    246,    324,\n",
      "              3,    937,    363,     66,      7,     71,     12,     82,      8,\n",
      "            357,    246,    324,      3,   1066,    363,     66,    357,    246,\n",
      "            417,   1332,   4810,    254,  13656,   3587,    395,    463,   3309,\n",
      "             16,     12,     66,      8,     82,    919,    496,    357,    246,\n",
      "            417,   1332,   6587,    395,    463,    363,     65,   4975,    252,\n",
      "            919,    258,      7,     71,     12,     82,      8,    357,    246,\n",
      "            415,   1332,   6587,    395,    463,    363,     65,    570,    252,\n",
      "            357,    246,    324,      3,    457,   6525,    280,    363,     50,\n",
      "              3,    285,   4117,    363,     66,      7,     71,     12,     82,\n",
      "              8,    357,    246,    324,   1332,   2158,   1296,  14115,   4317,\n",
      "            565,    395,    463,    363,     84,    403,    286,    570,    252,\n",
      "             11,    353,    403,    286,      6,    570,    252,    357,    246,\n",
      "            324,      3,    937,    363,     84,    919,    353,    357,    246,\n",
      "            324,   1332,  32966,   1572,    395,    463,    363,     84,    919,\n",
      "            353,    403,    286,    919,    286,      6,    570,    207,     17,\n",
      "             82,   1332,   6587,    276,   1296,    344,    437,    317,    279,\n",
      "            363,     50,      3,    395,    933,    276,   1296,    344,    363,\n",
      "             71,    919,    286,      6,    570,    252,    357,    246,    415,\n",
      "           1332,   4810,  13656,    285,    254,   1714,    344,    363,   5902,\n",
      "            417,    357,   9301,    207,     17,      3,    937,    344,   2456,\n",
      "           3309,     16,     12,     17,      8,     82,    919,    207,     17,\n",
      "             71,    403,    207,     17,     71,    570,     82,     59,    246,\n",
      "            415,      3,    285,   3309,     16,     12,     17,      8,     82,\n",
      "            919,    207,     17,     71,      6,    403,    207,     17,     71,\n",
      "              6,    570,    252,     59,    246,    415,   1332,   6587,   1962,\n",
      "            363,     17,    357,    246,    417,    357,    207,    185,    185,\n",
      "          77398,     25]])\n",
      " 2h -s, 2h' - s \\in H$ and $H$ is a Affine space. Thus $h + h' - s \\in S$ since it is an affine space. Thus $S$ is a subspace.\n",
      "Thus any point will work to define an affine set since translation of subspaces of $V$ give another subspace. So we can define $0 \\in S$ as any $s$ and $c \\cdot 0 = \n"
     ]
    }
   ],
   "source": [
    "\n",
    "prompt_input = r\"\"\"Pick any point $s$ in $H$ since it is not empty.\n",
    "Then we have to show that the set $S = \\{h - s: h \\in H \\}$ is a linear subspace.\n",
    "Then $0 \\in S$ since when $h = s $. For scalar multiplication we have to show that if\n",
    "$h - s \\in S$ then $c(h-s) \\in S$ where $c \\in F$. From the CL property we have $(1-c)s +\n",
    "ch \\in F$. Thus we have $b := s + c(h-s) \\in H$. Thus we have $b - s \\in S$ by definition of $S$\n",
    "and thus $c(h-s) \\in S$. To show scalar addition if we have $u = h - s, v = h' - s \\in S$ then\n",
    "$u + v \\in S$. Adding both we have $u + v = h + h' - 2s$. Thus to show that this is in $S$\n",
    "we need to show that $h + h' - s \\in H$. From CL and the fact that $char F \\neq 2$\n",
    "then that means $(1-2)s + 2h = 2h -s\\in H$ and $(1-2)s + 2h' = 2h' - s\\in H$.\n",
    "Thus since $2 \\in F \\ \"\"\"\n",
    "\n",
    "print(model_manager(prompt_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The statement is: \n",
      "  We need to show that if $H = \\{ x_0 + w: w \\in W\\}$ then $H = \\{ x_0' + w: w \\in W\\}$ for any $x_0' \\in H$. Then we have two cases. $W$ is the $0$ subspace or $W$ is a subspace with more than 1 element.\n",
      "If $W$ is a $0$ subspace, then $H = \\{ x_0 + 0: x_0 \\in V\\} = V$. In this case, any $x_0' \\in H$ satisfies $H = \\{ x_0' + w: w \\in W\\}$.\n",
      "If $W$ is a subspace with more than 1 element, then for any $x_0' \\in H$, we have $x_0' = x_0 + w$ for some $w \\in W$. Then $H = \\{ x_0 + w: w \\in W\\} = \\{ x_0' + (w - w): w \\in W\\} = \\{ x_0' + w: w \\in W\\}$.\n",
      "Therefore, the point $x_0$ is not unique.\n",
      "\n",
      "The answer is $\\boxed{True}$.\n"
     ]
    }
   ],
   "source": [
    "question = r\"\"\"\\item \\label{affine.subset} Let $V$ be a vector space. A \\emph{subset} $H$ of $V$ is called \\emph{affine} if there exists a subspace $W$ of $V$ and a point $\\vx_0 \\in H$ such that\n",
    "\\begin{equation} \\label{eq:affine}\n",
    "H = \\{ \\vx_0 + \\vw : \\vw \\in W \\}.\n",
    "\\end{equation}\n",
    "We say that $H$ is an affine subset \\emph{modelled} on the subspace $W$.\n",
    "\n",
    "\\textbf{Remark.} Some authors use the term ``affine subspace'' to denote an affine subset and ``linear subspace'' to denote the usual notion of subspace. We use affine subset to avoid confusion.\n",
    "\\begin{enumerate}[{$[$}a{$]$}]\n",
    "\\item Show that the point $\\vx_0$ is \\emph{not} unique. That is, show $H = \\{ \\vx'_0 + \\vw : \\vw \\in W \\}$ for any $\\vx_0' \\in H$.\n",
    "\"\"\"\n",
    "\n",
    "statement = r\"\"\" We need to show that if $H = \\{ x_0 + w: w \\in W\\}$ then $H = \\{ x_0' + w: w \\in W\\}$ for any $x_0' \\in H$. Then we have two cases. $W$ is the $0$ subspace or $W$ is a subspace with more than 1 element.\n",
    "If $W$ is a $0$   \"\"\"\n",
    "instruction = f\"There will be a quesiton and an incomplete proof. Please reason step by step to help complete the latex proof and put your final latex sentence within \\\\boxed{{}}. The question is {question}. The statement is: \\n\"\n",
    "messages = [\n",
    "    {\"role\": \"user\",\n",
    "     \"content\": f\"\"\"{instruction} {statement}\"\"\"\n",
    "    }]\n",
    "\n",
    "input_tensor = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\")\n",
    "outputs = model.generate(input_tensor.to(model.device), max_new_tokens=1000)\n",
    "\n",
    "result = tokenizer.decode(outputs[0][input_tensor.shape[1]:], skip_special_tokens=True)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: There will be a quesiton and an incomplete proof. Please reason step by step to help complete the latex proof and put your final latex sentence within \\boxed{}. The question is \\item \\label{affine.subset} Let $V$ be a vector space. A \\emph{subset} $H$ of $V$ is called \\emph{affine} if there exists a subspace $W$ of $V$ and a point $\\vx_0 \\in H$ such that\n",
      "\\begin{equation} \\label{eq:affine}\n",
      "H = \\{ \\vx_0 + \\vw : \\vw \\in W \\}.\n",
      "\\end{equation}\n",
      "We say that $H$ is an affine subset \\emph{modelled} on the subspace $W$.\n",
      "\n",
      "\\textbf{Remark.} Some authors use the term ``affine subspace'' to denote an affine subset and ``linear subspace'' to denote the usual notion of subspace. We use affine subset to avoid confusion.\n",
      "\\begin{enumerate}[{$[$}a{$]$}]\n",
      "\\item Show that the point $\\vx_0$ is \\emph{not} unique. That is, show $H = \\{ \\vx'_0 + \\vw : \\vw \\in W \\}$ for any $\\vx_0' \\in H$.\n",
      ". The statement is: \n",
      "  We need to show that if $H = \\{ x_0 + w: w \\in W\\}$ then $H = \\{ x_0' + w: w \\in W\\}$ for any $x_0' \\in H$. Then we have two cases. $W$ is the $0$ subspace or $W$ is a subspace with more than 1 element.\n",
      "If $W$ is a $0$   \n",
      "\n",
      "Assistant: The statement is: \n",
      "  We need to show that if $H = \\{ x_0 + w: w \\in W\\}$ then $H = \\{ x_0' + w: w \\in W\\}$ for any $x_0' \\in H$. Then we have two cases. $W$ is the $0$ subspace or $W$ is a subspace with more than 1 element.\n",
      "If $W$ is a $0$ subspace, then $H\n"
     ]
    }
   ],
   "source": [
    "result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 373])\n"
     ]
    }
   ],
   "source": [
    "print(input_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "overleafcopilot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
