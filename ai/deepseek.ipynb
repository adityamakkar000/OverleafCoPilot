{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u5/a26makka/.conda/envs/overleafCopilot/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/u5/a26makka/.conda/envs/overleafCopilot/lib/python3.13/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/u5/a26makka/.conda/envs/overleafCopilot/lib/python3.13/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Downloading shards: 100%|██████████| 2/2 [20:07<00:00, 603.84s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:35<00:00, 17.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The integral of x^2 from 0 to 2 is given by the antiderivative of x^2 evaluated at the endpoints 0 and 2.\n",
      "The antiderivative of x^2 is (1/3)x^3.\n",
      "So, the integral is (1/3)(2^3) - (1/3)(0^3) = (1/3)(8) - (1/3)(0) = 8/3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
    "\n",
    "model_name = \"deepseek-ai/deepseek-math-7b-instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "model.generation_config = GenerationConfig.from_pretrained(model_name)\n",
    "model.generation_config.pad_token_id = model.generation_config.eos_token_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Since $\\Omega$ is countable then that means there exists a mapping from $\\mathbb{N} \\mapsto \\Omega$. Therefore for each $\\omega \\in \\Omega$ we can write $A_1 \\mapsto \\omega_1$, $A\n"
     ]
    }
   ],
   "source": [
    "statement = r\"\"\"\n",
    "Since $\\Omega$ is countable then that means there exists a mapping from $\\mathbb{N} \\mapsto \\Omega$. Therefore for each $\\omega \\in \\Omega$ we can write $A_1 \\mapsto \\omega_1$, $A_2 \\mapsto \\omega_2$, $A_3 \\mapsto \\omega_3$, and so forth where $\\omega_i$ is the $i^{\\text{th}}$ element in $\\Omega$. Since $F_{\\omega_i}$ is countable then that means there exists a mapping from the natural numbers to $F_{\\omega_i}$. Therefore we can write  \"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\",\n",
    "     \"content\": f\"\"\"Please reason step by step to complete the latex proof and put your final latex answer within \\\\boxed{{}}. The statement is:\\n{statement}\"\"\"\n",
    "    }]\n",
    "\n",
    "input_tensor = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\")\n",
    "outputs = model.generate(input_tensor.to(model.device), max_new_tokens=50)\n",
    "\n",
    "result = tokenizer.decode(outputs[0][input_tensor.shape[1]:], skip_special_tokens=True)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import List, Dict\n",
    "import datasets as d\n",
    "import csv\n",
    "from omegaconf import MISSING\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DatasetConfig:\n",
    "    instruction: str = MISSING\n",
    "    inputPath: str = MISSING\n",
    "    outputPath: str = MISSING\n",
    "    cutoff: int = 7\n",
    "\n",
    "\n",
    "class DatasetProcessor:\n",
    "    def __init__(self, config: DatasetConfig):\n",
    "        self.config = config\n",
    "\n",
    "    def read_data(self) -> List[str]:\n",
    "        with open(self.config.inputPath, \"r\") as file:\n",
    "            data = file.read()\n",
    "        return self._preprocess_data(data)\n",
    "\n",
    "    def _preprocess_data(self, data: str) -> List[str]:\n",
    "        sentences = data.split(\".\")\n",
    "        return [\n",
    "            x.strip(\"\\n\\\\n\").replace(\"\\n\", \" \").replace(\"  \", \" \") for x in sentences\n",
    "        ]\n",
    "\n",
    "    def create_dataset(self, data: List[str]) -> List[Dict]:\n",
    "        dataset = []\n",
    "        length = len(data)\n",
    "        for i in range(length):\n",
    "            sentence = data[i]\n",
    "            if len(sentence) - 1 < self.config.cutoff:\n",
    "                continue\n",
    "\n",
    "            indexes = [x for x in range(self.config.cutoff, len(sentence) - 1)]\n",
    "            input = [sentence[i:_] for _ in indexes]\n",
    "            output = [sentence[_:] for _ in indexes]\n",
    "            for a, b in zip(input, output):\n",
    "                dataset.append(\n",
    "                    {\n",
    "                        \"instruction\": f\"{self.config.instruction}\",\n",
    "                        \"input\": r\"{}\".format(a),\n",
    "                        \"output\": r\"{}\".format(b),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        return dataset\n",
    "\n",
    "    def save_dataset(self, dataset: List[Dict]) -> None:\n",
    "        with open(\n",
    "            self.config.outputPath, mode=\"w\", newline=\"\", encoding=\"utf-8\"\n",
    "        ) as file:\n",
    "            writer = csv.DictWriter(file, fieldnames=[\"instruction\", \"input\", \"output\"])\n",
    "            writer.writeheader()\n",
    "            writer.writerows(dataset)\n",
    "\n",
    "\n",
    "def process_dataset(config: DatasetConfig) -> None:\n",
    "    processor = DatasetProcessor(config)\n",
    "    data = processor.read_data()\n",
    "    dataset = processor.create_dataset(data)\n",
    "    processor.save_dataset(dataset)\n",
    "\n",
    "\n",
    "def get_dataset(config: DatasetConfig):\n",
    "    process_dataset(config)\n",
    "    ds = d.load_dataset(config.outputPath[-3:], data_files=config.outputPath)\n",
    "    return ds\n",
    "\n",
    "\n",
    "dataCFG = {\n",
    "    \"instruction\": \"Please reason step by step to complete the latex proof and put your final latex answer within \\\\boxed{{}}. The statement is:\\n\",\n",
    "    \"inputPath\": \"./dataset/latex.txt\",\n",
    "    \"outputPath\": \"./dataset/latex.csv\",\n",
    "    \"cutoff\": 7,\n",
    "}\n",
    "\n",
    "cfg = DatasetConfig(\n",
    "    instruction=dataCFG[\"instruction\"],\n",
    "    inputPath=dataCFG[\"inputPath\"],\n",
    "    outputPath=dataCFG[\"outputPath\"],\n",
    ")\n",
    "\n",
    "process_dataset(cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import LoraConfig, PeftModel, get_peft_model\n",
    "from trl import SFTTrainer\n",
    "import hydra\n",
    "from hydra.core.config_store import ConfigStore\n",
    "from omegaconf import OmegaConf, MISSING\n",
    "from ds import get_dataset, DatasetConfig\n",
    "from dataclasses import dataclass\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "class ModelTrainer:\n",
    "    def __init__(self, cfg: ftConfig):\n",
    "        self.cfg = cfg\n",
    "        self.device = self._get_device()\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "\n",
    "    def _get_device(self):\n",
    "        if torch.backends.mps.is_available():\n",
    "            return \"mps\"\n",
    "        return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def setup_output_directory(self):\n",
    "        if os.path.exists(f\"{self.cfg.output}/{self.cfg.name}/config.yaml\"):\n",
    "            if self.cfg.overwrite:\n",
    "                shutil.rmtree(f\"{self.cfg.output}/{self.cfg.name}/\")\n",
    "            else:\n",
    "                raise FileExistsError(\n",
    "                    \"Output directory exists. Set overwrite to true to overwrite.\"\n",
    "                )\n",
    "\n",
    "        os.makedirs(f\"./{self.cfg.output}/{self.cfg.name}/\", exist_ok=True)\n",
    "        with open(f\"./{self.cfg.output}/{self.cfg.name}/config.yaml\", \"w\") as file:\n",
    "            file.write(OmegaConf.to_yaml(self.cfg))\n",
    "\n",
    "    def prepare_dataset(self):\n",
    "        def generate_prompt(data_point):\n",
    "            prefix_text = self.cfg.prefix_txt\n",
    "            text = r\"\"\" <start_of_turn>user {prefix_text} {instruction}  {input} <end_of_turn> <start_of_turn>model {output} <end_of_turn>\"\"\".format(\n",
    "                prefix_text=prefix_text,\n",
    "                instruction=data_point[\"instruction\"],\n",
    "                input=data_point[\"input\"],\n",
    "                output=data_point[\"output\"],\n",
    "            )\n",
    "\n",
    "            return text\n",
    "\n",
    "        ds = get_dataset(self.cfg.dataCFG)\n",
    "        text_column = [generate_prompt(data_point) for data_point in ds[\"train\"]]\n",
    "        ds = ds[\"train\"].add_column(\"prompt\", text_column)\n",
    "        ds = ds.shuffle(seed=self.cfg.seed)\n",
    "        ds = ds.map(lambda samples: self.tokenizer(samples[\"prompt\"]), batched=True)\n",
    "        ds = ds.train_test_split(test_size=self.cfg.test_size)\n",
    "        return ds[\"train\"], ds[\"test\"]\n",
    "\n",
    "    def setup_model(self):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            self.cfg.model_id, add_eos_token=True, padding_side=\"left\"\n",
    "        )\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.cfg.model_id, device_map=self.device\n",
    "        )\n",
    "        self.model.gradient_checkpointing_enable()\n",
    "\n",
    "        modules = self._find_all_linear_names()\n",
    "        target_modules = (\n",
    "            modules\n",
    "            if len(modules) < self.cfg.modules_limit\n",
    "            else modules[: self.cfg.modules_limit]\n",
    "        )\n",
    "\n",
    "        lora_config = LoraConfig(\n",
    "            r=self.cfg.r,\n",
    "            lora_alpha=self.cfg.lora_alpha,\n",
    "            target_modules=target_modules,\n",
    "            lora_dropout=0.05,\n",
    "            bias=\"none\",\n",
    "            task_type=\"CAUSAL_LM\",\n",
    "        )\n",
    "\n",
    "        self.model = get_peft_model(self.model, lora_config)\n",
    "        return lora_config\n",
    "\n",
    "    def _find_all_linear_names(self):\n",
    "        lora_module_names = set()\n",
    "        for name, module in self.model.named_modules():\n",
    "            if isinstance(module, torch.nn.Linear):\n",
    "                names = name.split(\".\")\n",
    "                lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "        return list(lora_module_names)\n",
    "\n",
    "    def train(self):\n",
    "        self.setup_output_directory()\n",
    "        lora_config = self.setup_model()\n",
    "        train_data, test_data = self.prepare_dataset()\n",
    "\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        self.tokenizer.padding_side = \"right\"\n",
    "\n",
    "        trainer = SFTTrainer(\n",
    "            model=self.model,\n",
    "            train_dataset=train_data,\n",
    "            eval_dataset=test_data,\n",
    "            dataset_text_field=\"prompt\",\n",
    "            peft_config=lora_config,\n",
    "            max_seq_length=250,\n",
    "            args=transformers.TrainingArguments(\n",
    "                per_device_train_batch_size=self.cfg.per_device_train_batch_size,\n",
    "                gradient_accumulation_steps=self.cfg.gradient_accumulation_steps,\n",
    "                warmup_steps=self.cfg.warmup_steps,\n",
    "                max_steps=self.cfg.max_steps,\n",
    "                learning_rate=self.cfg.learning_rate,\n",
    "                logging_steps=self.cfg.logging_steps,\n",
    "                output_dir=f\"{self.cfg.output}/{self.cfg.name}/checkpoints\",\n",
    "                optim=self.cfg.optim,\n",
    "                save_strategy=\"epoch\",\n",
    "            ),\n",
    "            data_collator=transformers.DataCollatorForLanguageModeling(\n",
    "                self.tokenizer, mlm=False\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        trainer.train()\n",
    "        self.save_models(trainer)\n",
    "\n",
    "    def save_models(self, trainer):\n",
    "        new_model_path = f\"{self.cfg.output}/{self.cfg.name}/finetuned_models/\"\n",
    "        trainer.model.save_pretrained(new_model_path)\n",
    "\n",
    "        merged_model = PeftModel.from_pretrained(self.model, new_model_path)\n",
    "        merged_model = merged_model.merge_and_unload()\n",
    "\n",
    "        merged_path = f\"{self.cfg.output}/{self.cfg.name}/merged_models/\"\n",
    "        merged_model.save_pretrained(merged_path)\n",
    "        self.tokenizer.save_pretrained(merged_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "overleafCopilot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
